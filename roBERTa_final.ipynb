{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# !pip install contractions\n",
    "import contractions\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('final_train.txt', names=['text', 'emotion'], sep=';')\n",
    "val_data = pd.read_csv('final_val.txt', names=['text', 'emotion'], sep=';')\n",
    "test_data = pd.read_csv('final_test.txt', names=['text', 'emotion'], sep=';')\n",
    "train_data.head()\n",
    "\n",
    "train_data['emotion'] = train_data['emotion'].replace('love', 'trust')\n",
    "test_data['emotion'] = test_data['emotion'].replace('love', 'trust')\n",
    "val_data['emotion'] = val_data['emotion'].replace('love', 'trust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Train Data': train_data, 'Validation Data': val_data, 'Test Data': test_data}\n",
    "for temp in data:\n",
    "    print(temp)\n",
    "    print(data[temp].isnull().sum())\n",
    "    print('*'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar, ax = plt.subplots(1,3, figsize=(30, 10))\n",
    "for index, temp in enumerate(data):\n",
    "    sns.countplot(ax = ax[index],x = 'emotion', data = data[temp])\n",
    "    ax[index].set_title(temp+' Class Frequency', size=14)\n",
    "    ax[index].set_ylabel('Frequency', size=14)\n",
    "    ax[index].set_xlabel(temp+' Class', size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cloud(wordcloud, temp):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title(temp+' Word Cloud', size = 16)\n",
    "    plt.imshow(wordcloud) \n",
    "    # No axis details\n",
    "    plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in data:\n",
    "    temp_text = ' '.join([sentence for sentence in data[temp].text])\n",
    "    wordcloud = WordCloud(width = 600, height = 600).generate(temp_text)\n",
    "    plot_cloud(wordcloud, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = re.sub('[^A-z]', ' ', sentence)\n",
    "    negative = ['not', 'neither', 'nor', 'but', 'however', 'although', 'nonetheless', 'despite', 'except',\n",
    "                        'even though', 'yet']\n",
    "    stop_words = [z for z in stop_words if z not in negative]\n",
    "    preprocessed_tokens = [lemmatizer.lemmatize(contractions.fix(temp.lower())) for temp in sentence.split() if temp not in stop_words] #lemmatization\n",
    "    return ' '.join([x for x in preprocessed_tokens]).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text'] = train_data['text'].apply(lambda x: preprocess(x))\n",
    "val_data['text'] = val_data['text'].apply(lambda x: preprocess(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "train_x, train_y = ros.fit_resample(np.array(train_data['text']).reshape(-1, 1), np.array(train_data['emotion']).reshape(-1, 1))\n",
    "train = pd.DataFrame(list(zip([x[0] for x in train_x], train_y)), columns = ['text', 'emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.OneHotEncoder()\n",
    "y_train= le.fit_transform(np.array(train['emotion']).reshape(-1, 1)).toarray()\n",
    "y_test= le.fit_transform(np.array(test_data['emotion']).reshape(-1, 1)).toarray()\n",
    "y_val= le.fit_transform(np.array(val_data['emotion']).reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_encode(data,maximum_length) :\n",
    "  input_ids = []\n",
    "  attention_masks = []\n",
    "  \n",
    "\n",
    "  for i in range(len(data.text)):\n",
    "      encoded = tokenizer.encode_plus(\n",
    "        \n",
    "        data.text[i],\n",
    "        add_special_tokens=True,\n",
    "        max_length=maximum_length,\n",
    "        pad_to_max_length=True,\n",
    "        \n",
    "        return_attention_mask=True,\n",
    "        \n",
    "      )\n",
    "      \n",
    "      input_ids.append(encoded['input_ids'])\n",
    "      attention_masks.append(encoded['attention_mask'])\n",
    "  return np.array(input_ids),np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(x.split()) for x in train_data['text']])\n",
    "train_input_ids,train_attention_masks = roberta_encode(train, max_len)\n",
    "test_input_ids,test_attention_masks = roberta_encode(test_data, max_len)\n",
    "val_input_ids,val_attention_masks = roberta_encode(val_data, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bert_model, max_len):\n",
    "    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
    "    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
    "\n",
    "    output = bert_model([input_ids,attention_masks])\n",
    "    output = output[1]\n",
    "\n",
    "    output = tf.keras.layers.Dense(6, activation='softmax')(output)\n",
    "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n",
    "    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFRobertaModel\n",
    "roberta_model = TFRobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(roberta_model, max_len)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([train_input_ids,train_attention_masks], y_train, validation_data=([val_input_ids,val_attention_masks], y_val), epochs=4,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict([test_input_ids,test_attention_masks])\n",
    "y_pred = np.zeros_like(result)\n",
    "y_pred[np.arange(len(result)), result.argmax(1)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy ', accuracy)\n",
    "f1 = f1_score(y_test, y_pred, average = 'macro')\n",
    "print('F1 Score :', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "model.save_weights('roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(result):\n",
    "    sns.barplot(x = 'Category', y = 'Confidence', data = result)\n",
    "    plt.xlabel('Categories', size=14)\n",
    "    plt.ylabel('Confidence', size=14)\n",
    "    plt.title('Emotion Classification', size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_inference_encode(data,maximum_length) :\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "  \n",
    "\n",
    "  \n",
    "    encoded = tokenizer.encode_plus(\n",
    "    data,\n",
    "    add_special_tokens=True,\n",
    "    max_length=maximum_length,\n",
    "    pad_to_max_length=True,\n",
    "\n",
    "    return_attention_mask=True\n",
    "\n",
    "    )\n",
    "\n",
    "    input_ids.append(encoded['input_ids'])\n",
    "    attention_masks.append(encoded['attention_mask'])\n",
    "    return np.array(input_ids),np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text_sentence, max_len):\n",
    "    preprocessed_text = preprocess(text_sentence)\n",
    "    input_ids, attention_masks = roberta_inference_encode(preprocessed_text, maximum_length = max_len)\n",
    "    model = create_model(roberta_model, 43)\n",
    "    model.load_weights('my_checkpoint')\n",
    "    result = model.predict([input_ids, attention_masks])\n",
    "    #le.categories_[0] = ['anger' 'fear' 'joy' 'love' 'sadness' 'surprise']\n",
    "    result = pd.DataFrame(dict(zip(list(le.categories_[0]), [round(x*100, 2)for x in result[0]])).items(), columns = ['Category', 'Confidence'])\n",
    "    plot_result(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = inference(\"I am unhappy\", max_len)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
